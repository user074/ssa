{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a quick start guide for starting training the SSA RL version. Because of the VRAM limitation, we use the Qwen2.5-0.5B model. It would still require 40GB A100 on colab. In addition, for the demo purpose, we set the data shuffle to False so it can just run some steps for GSM8K, which is much shorter than MATH solutions. Please set it back to True during actual training! And in that case you might need a GPU with more VRAM (probbaly 48GB+). Because the MATH solutions are longer, it increases the VRAM usage that 40GB A100 will have OOM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Install dependencies\n",
    "It will restart the runtime after the installtion on the colab. So please run the step 2 after the installation is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(f\"Running in Google Colab: {IN_COLAB}\")\n",
    "    print(\"Installing repo\")\n",
    "    !git clone https://github.com/user074/ssa.git\n",
    "    %cd ssa-main\n",
    "\n",
    "    # Read and clean the requirements file\n",
    "    with open('requirements_colab.txt', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Filter out conda-specific paths and keep only standard package specs\n",
    "    clean_lines = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line.startswith('#') and not '/home/conda/' in line and not 'file://' in line:\n",
    "            # Extract just the package name and version if it's a standard format\n",
    "            if '==' in line:\n",
    "                clean_lines.append(line.split()[0])  # Take just the package==version part\n",
    "\n",
    "    # Create a new clean requirements file\n",
    "    with open('requirements_clean.txt', 'w') as f:\n",
    "        f.write('\\n'.join(clean_lines))\n",
    "\n",
    "    # Now install from the clean file\n",
    "    !pip install -r requirements_clean.txt\n",
    "\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(f\"Running in Google Colab: {IN_COLAB}\")\n",
    "    !conda env create -f environment.yml\n",
    "    !conda activate SSA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Continue the installation\n",
    "It will restart the runtime. Then run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    %cd ssa-main\n",
    "except:\n",
    "    pass\n",
    "!git clone https://github.com/openai/prm800k\n",
    "%cd torchtune\n",
    "!pip install -e .\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the demo purpose, we use the Qwen2.5-0.5B model. You can download it from huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"huggingface_hub[hf_transfer]\"\n",
    "!HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download Qwen/Qwen2.5-0.5B --local-dir model/Qwen2.5-0.5B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the dataset, we will use our existing cleaned dataset from huggingface, which is `user074/concat_cleaned_gsm8k_math_5`. The dataset is already cleaned and ready to use. We prepared a config file for the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First login to wandb\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of the demo results from wandb log. We can see the success rate goes up very quickly even with only 200 steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](figures/demo-results.png \"Demo 200 steps\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training now! You can see the log of each step below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tune run --nproc_per_node 1 dev/grpo_full_finetune_distributed --config ./05B_rl_SSA_qwen.yaml"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
